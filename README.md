# ğŸ“© Spam Classification Using CNN

## ğŸ§­ Overview
This project focuses on detecting **spam messages in Email data** using **Deep Learning**.  
The goal is to automatically classify messages as *spam* or *ham* (non-spam) based on their textual content.

After experimenting with traditional preprocessing and tokenization methods, I applied a **1D Convolutional Neural Network (Conv1D CNN)** to learn meaningful patterns in the text data.  
The model successfully captures contextual word relationships and achieves **high accuracy in spam detection**.

---

## âš™ï¸ How It Works

### ğŸ§¹ 1. Data Preprocessing
- Unnecessary columns were dropped, and labels were mapped to:
  - `0` â†’ ham (non-spam)
  - `1` â†’ spam  
- **Text cleaning**:
  - Tokenization with **NLTK**
  - Removal of English **stopwords**
  - Conversion of text into numeric **sequences** using Keras `Tokenizer`
  - Padding sequences to equal lengths with `pad_sequences`

### âš–ï¸ 2. Handling Class Imbalance
- The dataset was imbalanced (more *ham* than *spam* messages).
- Used **SMOTE (Synthetic Minority Oversampling Technique)** to generate synthetic spam samples, balancing the dataset before training.

### ğŸ§  3. CNN Model Architecture
The model is built using **Keras Sequential API**:
1. **Embedding Layer** â€“ converts word indices into dense vector representations.  
2. **Conv1D Layers** â€“ slide multiple filters (kernels) across sequences to extract n-gram features and local patterns.  
3. **MaxPooling & GlobalMaxPooling** â€“ reduce dimensionality and focus on the most important features.  
4. **Batch Normalization** â€“ stabilizes learning and accelerates convergence.  
5. **Dropout** â€“ prevents overfitting by randomly dropping neurons during training.  
6. **Dense Layers** â€“ learn higher-level representations.  
7. **Sigmoid Output Layer** â€“ outputs probability between 0 (ham) and 1 (spam).

The **ReLU activation function** was used throughout hidden layers for efficiency and non-linearity.  
The **Adam optimizer** combined the benefits of momentum and adaptive learning rate to speed up convergence.

---

## ğŸ§© Technologies Used
| Library / Tool | Purpose |
|----------------|----------|
| **Pandas** | Handle and clean tabular data |
| **NumPy** | Matrix and numerical operations |
| **Matplotlib** | Data visualization (accuracy/loss graphs) |
| **NLTK** | Text tokenization and stopword removal |
| **SMOTE (imblearn)** | Handle class imbalance |
| **TensorFlow / Keras** | Deep Learning framework for building CNN |
| **Scikit-learn** | Train-test split and utility functions |

---

## ğŸ“ˆ Model Performance
- Training and validation accuracy/loss were tracked across epochs.
- After 4 epochs, the model reached **strong accuracy** on both training and test sets, demonstrating that CNNs effectively learn spam-related word patterns.

---

## ğŸ“Š Results Visualization
Training and validation performance were visualized using Matplotlib:

- **Left:** Training vs. Validation Accuracy  
- **Right:** Training vs. Validation Loss  

The plots clearly show stable learning and no major overfitting issues.

---

## ğŸš€ Key Takeaways
- Deep Learning (CNN) can handle **text classification** tasks effectively â€” even though CNNs are more common in image tasks.  
- Combining **tokenization, embeddings, and convolution** layers allows the model to extract **contextual patterns** from sequences.  
- **SMOTE** plays an important role in ensuring balanced learning.  
- **Dropout** and **Batch Normalization** greatly improve generalization.

---
